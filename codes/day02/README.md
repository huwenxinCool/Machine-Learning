# 分类算法
- sklearn的转换器和估计器
	- 什么是转换器
		- 就是特征工程的接口称为转换器， 其中转换器分为这么几种
		- fit_transform
		- fit
		- transform	

	- 什么是估计器
	- 估计器（estimator)分为几种
	- 用于分类的估计器
		- sklearn.neighbors K近邻算法
		- sklearn.naive_bayes 贝叶斯
		- sklearn.linear_model.LogisticRegression 逻辑回归
		- sklearn.tree 决策树与随机森林
	- 用于回归的估计器
		- sklearn.linear_model.LinearRegression 线性回归
		- sklearn.linear_model.Ridge 岭回归
	- 用于无监督学习的估计器
		- sklearn.cluster.KMeans 聚类

### K-近邻算法（KNN)原理
- 定义
	- 如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。
- 列如：
	- a(a1, a2, a3), b(b1, b2, b3)
	- 公式：（（a1 - b1）*（a1 - b1）+ （a2 - b2）*（a2 - b2） + （a3 - b3）*（a3 - b3）） ** 0.5
	- 就是距离公式
- API 
	- sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, algorithm="auto")
	- n_neighbors：int,可选（默认= 5），k_neighbors查询默认使用的邻居数
	- algorithm：{‘auto’，‘ball_tree’，‘kd_tree’，‘brute’}，可选用于计算最近邻居的算法：‘ball_tree’将会使用 BallTree，‘kd_tree’将使用 KDTree。‘auto’将尝试根据传递给fit方法的值来决定最合适的算法。 (不同实现方式影响效率)
- 案列1： 鸢尾花种类预测
- 案例2：预测签到位置
	- 发现正确率只有0.4 明显就不可以  需要另外一种解决办法

## 模型选择和调优
- 交叉验证
	- 什么叫交叉验证？
	- 就是从训练集分为几部分的训练集， 然后从每部分训练集拿出一部分做为验证集。得到各个部分的模型的结果，去平均值作为最终结果
	- 为什么要使用交叉验证？
		- 就是得到更可靠的模型
- 超参数搜索-网格搜索(Grid Search)
	- 有很多参数是需要手动指定的（如k-近邻算法中的K值），这种叫超参数
	- 但是手动过程繁杂，所以需要对模型预设几种超参数组合。每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建立模型。
- 模型选择与调优API
	- sklearn.model_selection.GridSearchCV(estimator, param_grid=None, cv=None)
		- 对估计器指定参数进行详尽搜索
		- estimator: 估计器对象
		- param_grid: 估计器对象的参数（dict){'n_neighbors': [1, 3, 5]}
		- cd: 指定几折交叉验证
		- fit: 输入训练集
		- score: 准确率
		- 结果分析：
			- bestscore:在交叉验证中验证的最好结果_
			- bestestimator：最好的参数模型
			- cvresults:每次交叉验证后的验证集准确率结果和训练集准确率结果
	- 对案列2进行交叉验证。提高模型的准确率
	- 对案列1进行交叉验证。发现模型的正确率为1
		- estimator.score 对模型的评估
		- estimator.predict 对测试的检查
		- estimatot.best_score_ 最好的模型结果
		- estiator.best_estimator_ 最好的模型参数
		- estimator.cv_results_ 每次交叉验证后的验证集准确率结果和训练集准确率结果
- 总结：
	- 交叉验证与网络搜索进行模型参数的选择	

## 朴素贝叶斯算法
- 什么是朴素贝叶斯分类算法？
	- 朴素贝叶斯发源于古代的数学理论，有着坚实的数学基础，以及稳定的分类效率
	- 朴树贝叶斯基于一个简单的假设：给定目标值时属性之间相互独立

- 概率基础
	- 联合概率： 包含多个条件，且所有条件同时成立的概率
		- 记作：P(A,B)
		- 特性：P(A, B) = P(A)P(B)
	- 条件概率： 就是事件A在另外一个事件B已经发生条件下的发生概率
		- 记作：P(A|B)
		- 特性：P(A1,A2|B) = P(A1|B)P(A2|B)
	- 相互独立： 如果P(A, B) = P(A)P(B)，则称事件A与事件B相互独立。
- API
	- sklearn.naive_bayes.MultionmialNB(alpha=1.0)
		- alpha:拉普拉斯平滑系数
- 案列3：20类新闻的分类

## 决策树
- 决策树的思想特别朴素，就是利用条件语句if-then的结构，只是在这里是把最重要的条件放在最上面
- 原理：信息熵，信息增益等
- 决策树的划分依据之一     信息增益
	- ID3
		- 信息增益 最大的准则
	- C4.5
		- 信息增益比 最大的准则
	- CART
		- 分类树: 基尼系数 最小的准则 在sklearn中可以选择划分的默认原则
		- 优势：划分更加细致（从后面例子的树显示来理解）
- API
	- sklearn.tree.DecisionTreeClassifier(criterion='gini', max_depth=None, random_state=None)
	- 决策树分类器
	- criterion: 默认gini系数， 也可以选择信息增益的熵
	- max_depth: 树的最大深度
	- random_state 随机数种子
- 案例：泰坦尼克号乘客生存预测
- 决策树总结
- 优点：
	- 简单的理解和解释， 树木可视化
- 缺点：
	- 决策树学习者可以创建不能很好地推广数据过于复杂的树，这被称为过拟合
- 改进：
	- 减枝cart算法（决策树API当中已经实现， 随机森林参数调优有相关操作）
	- 随机森林

## 集成学习方法之随机森林
- 什么是集成学习方法
	- 集成学习通过建立几个模型组合的来解决单一预测问题， 它的工作原理是生成多个分类器|模型，各自独立地学习和作出预测。这些预测最好结合成组合预测，因此优于任何一个单分类的做出预测
- 什么是随机森林
	- 在机器学习中， 随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定
- 随机森林原理过程
	- 用N来表示训练用例（样本）的个数， M表示特征数目
		- 1. 一次随机选出一个样本，重复N次（有可能出现先重复的样本）
		- 2. 随机去选出m个特征， m << M 建立决策树
		- 采用bootstrap抽样

	- 为什么采用bootstrap抽样
		- 为什么要随机抽样训练集？
			- 如果不进行随机抽样， 没棵树的训练集都一样， 那么最终训练出的树分类结果也是完全一样
		- 为什么要有放回地抽样?
			- 如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是“有偏的”，都是绝对“片面的”（当然这样说可能不对），也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决。